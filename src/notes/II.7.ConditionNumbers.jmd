## II.7 Condition numbers

We have seen that floating point arithmetic induces errors in computations, and that we can typically
bound the absolute errors to be proportional to $C œµ_{\rm m}$. We want a way to bound the
effect of more complicated calculations like computing $A ùê±$ or $A^{-1} ùê≤$ without having to deal with
the exact nature of floating point arithmetic. Here we consider only matrix-multiplication but will make a remark
about matrix inversion.

To justify what follows, we first observe that errors in implementing matrix-vector multiplication
can be captured by considering the multiplication to be exact on the wrong matrix: that is, `A*x`
(implemented with floating point) is precisely $A + Œ¥A$ where $Œ¥A$ has small norm, relative to $A$.
This is known as _backward error analysis_.



To discuss floating point errors we need to be precise which order the operations happened.
We will use the definition `mul(A,x)`, which denote ${\rm mul}(A, ùê±)$. (Note that `mul_rows` actually
does the _exact_ same operations, just in a different order.) Note that each entry of the result is in fact a dot-product
of the corresponding rows so we first consider the error in the dot product  `dot(ùê±,ùê≤)` as implemented in floating-point, 
which we denote ${\rm dot}(A,x)$.

We first need a helper proposition, from PS2 Q2.1:

**Proposition 3** If $|œµ_i| ‚â§ œµ$ and $n œµ < 1$, then
$$
\prod_{k=1}^n (1+œµ_i) = 1+Œ∏_n
$$
for some constant $Œ∏_n$ satisfying $|Œ∏_n| ‚â§ E_{n,œµ} := {n œµ \over 1-nœµ}$.



**Lemma 1 (dot product backward error)**
For $ùê±, ùê≤ ‚àà ‚Ñù^n$,
$$
{\rm dot}(ùê±, ùê≤) = (ùê± + Œ¥ùê±)^‚ä§ ùê≤
$$
where
$$
|Œ¥ùê±| ‚â§¬† E_{n,œµ_{\rm m}/2} |ùê± |,
$$
where $|ùê± |$ means absolute-value of each entry, assuming $n œµ_{\rm m} < 2$.


**Proof**

This is related to PS2 Q2.3 but asks for the _backward error_ instead of the
_forward error_. Note
$$
{\rm dot}(ùê±, ùê≤) = ‚®Å_{j=1}^n (x_j ‚äó y_j) = ‚®Å_{j=1}^n (x_j  y_j) (1 + Œ¥_j)
= x_1 y_1 (1 + Œ∏ÃÉ_{n}) +   ‚àë_{j=2}^n x_j y_j (1 + Œ∏_{n-j+2})
$$
where $|Œ∏ÃÉ_n|, |Œ∏_k| ‚â§¬†E_{n,œµ_{\rm m}/2}$ (the subscript denotes the number of terms
bounded by $Œµ_{\rm m}/2$. Thus we can define
$$
Œ¥ùê±  := \begin{bmatrix}
x_1 Œ∏ÃÉ_n \\
x_2 Œ∏_n \\
‚ãÆ \\
x_n Œ∏_2
\end{bmatrix}
$$
where
$$
| Œ¥ùê± | ‚â§¬†E_{n,œµ_{\rm m}/2} | ùê± |.
$$


‚àé

**Theorem 3 (matrix-vector backward error)**
For $A ‚àà ‚Ñù^{m √ó n}$ and $ùê± ‚àà ‚Ñù^n$ we have
$$
{\rm mul_rows}(A, ùê±) = (A + Œ¥A) ùê±
$$
where
$$
|Œ¥A| ‚â§ E_{n,œµ_{\rm m}/2}  |A|,
$$
assuming $n œµ_{\rm m} < 2$. Therefore
$$
\begin{align*}
\|Œ¥A\|_1 &‚â§¬† E_{n,œµ_{\rm m}/2} \|A \|_1 \\
\|Œ¥A\|_2 &‚â§¬† \sqrt{\min(m,n)} E_{n,œµ_{\rm m}/2} \|A \|_2 \\
\|Œ¥A\|_‚àû &‚â§¬† E_{n,œµ_{\rm m}/2} \|A \|_‚àû
\end{align*}
$$

**Proof**
The bound on $|Œ¥A|$ is implied by the previous lemma.
The $1$ and $‚àû$-norm follow since
$$
\|A\|_1 = \||A|\|_1 \hbox{ and } \|A\|_‚àû = \||A|\|_‚àû
$$
This leaves the 2-norm example, which is a bit more challenging as there are matrices
$A$ such that $\|A\|_2 ‚â†¬†\| |A| \|_2$.
Instead we will prove the result by going through the Fr√∂benius norm and using:
$$
\|A \|_2 ‚â§ \|A\|_F ‚â§¬†\sqrt{r} \| A\|_2
$$
where $r$ is rank of $A$ (see PS6 Q5.2)
and $\|A\|_F = \| |A| \|_F$,
so we deduce:
$$
\begin{align*}
\|Œ¥A \|_2 &‚â§¬†\| Œ¥A\|F = \| |Œ¥A| \|F ‚â§¬†E_{n,œµ_{\rm m}/2} \| |A| \|_F \\
          &= E_{n,œµ_{\rm m}/2} \| A \|_F ‚â§¬†\sqrt{r} E_{n,œµ_{\rm m}/2} \| A \|_2 \\
          &‚â§ \sqrt{\min(m,n)} E_{n,œµ_{\rm m}/2} \|A \|_2
\end{align*}
$$

‚àé

So now we get to a mathematical question independent of floating point: 
can we bound the _relative error_ in approximating
$$
A ùê± ‚âà (A + Œ¥A) ùê±
$$
if we know a bound on $\|Œ¥A\|$?
It turns out we can in turns of the _condition number_ of the matrix:

**Definition 2 (condition number)**
For a square matrix $A$, the _condition number_ (in $p$-norm) is
$$
Œ∫_p(A) := \| A \|_p \| A^{-1} \|_p
$$
with the $2$-norm:
$$
Œ∫_2(A) = {œÉ_1 \over œÉ_n}.
$$


**Theorem 4 (relative-error for matrix-vector)**
The _worst-case_ relative error in $A ùê± ‚âà (A + Œ¥A) ùê±$ is
$$
{\| Œ¥A ùê± \| \over \| A ùê± \| } ‚â§ Œ∫(A) Œµ
$$
if we have the relative pertubation error $\|Œ¥A\| = \|A \| Œµ$.

**Proof**
We can assume $A$ is invertible (as otherwise $Œ∫(A) = ‚àû$). Denote $ùê≤ = A ùê±$ and we have
$$
{\|ùê± \| \over \| A ùê± \|} = {\|A^{-1} ùê≤ \| \over \|ùê≤ \|} ‚â§¬†\| A^{-1}\|
$$
Thus we have:
$$
{\| Œ¥A ùê± \| \over \| A ùê± \| } ‚â§ \| Œ¥A\| \|A^{-1}\| ‚â§¬†Œ∫(A) {\|Œ¥A\| \over \|A \|}
$$

‚àé


Thus for floating point arithmetic we know the error is bounded by $Œ∫(A) E_{n,œµ_{\rm m}/2}$.

If one uses QR to solve $A ùê± = ùê≤$ the condition number also gives a meaningful bound on the error. 
As we have already noted, there are some matrices where PLU decompositions introduce large errors, so
in that case well-conditioning is not a guarantee (but it still usually works).
